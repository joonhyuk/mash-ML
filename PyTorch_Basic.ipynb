{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch with Jupyter Notebook\n",
    "- 구글 코랩을 쓰는게 나을지 이게 나을지\n",
    "    - 코랩은 어지간한 패키지 내장된 듯. code에서는 venv 내에서 pip 써야 함\n",
    "- md 부분은 md 확장이 먹는 것 같다 더 낫구만.\n",
    "    - 이걸로 갑시다.\n",
    "- 드디어 나도 주피터 노트북 사용자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기초공사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 준비물들\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from varname import nameof\n",
    "\n",
    "def print_info(tensor:torch.FloatTensor):\n",
    "    try:\n",
    "        name_of_var = nameof(tensor, frame = 2)\n",
    "    except:\n",
    "        name_of_var = 'tensor'\n",
    "    print(f'Rank(dim) of {name_of_var} is {tensor.dim()}')\n",
    "    print(f'Size of {name_of_var} is {tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 조작 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t1 = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(ndim) of t1 is 1\n",
      "shape of t1 is (7,)\n"
     ]
    }
   ],
   "source": [
    "print(f'rank(ndim) of t1 is {t1.ndim}')\n",
    "print(f'shape of t1 is {t1.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape에서 `(7,)`는 `(1,7)`과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t2 = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank(ndim) of t2 is 2\n",
      "shape of t2 is (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'rank(ndim) of t2 is {t2.ndim}')\n",
    "print(f'shape of t2 is {t2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch를 한번 써보자\n",
    "- 일단은 `FloatTensor`를 주로 쓸 듯\n",
    "- `dim()`과 `shape`가 있고, `shape`는 `size()`와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n",
      "Rank(dim) of pt1 is 1\n",
      "Size of pt1 is torch.Size([7])\n",
      "<class 'torch.Size'>\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "pt1 = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(pt1)\n",
    "print_info(pt1)\n",
    "print(type(pt1.shape))\n",
    "print(pt1.shape == pt1.size())\n",
    "print(pt1.shape is pt1.size()) # 다른 인스턴스이므로 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "Rank(dim) of pt2 is 2\n",
      "Size of pt2 is torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "pt2 = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "print(pt2)\n",
    "print_info(pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  5.,  8., 11.])\n",
      "tensor(5.)\n",
      "tensor([[ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(pt2[:, 1])\n",
    "print(pt2[1, 1])\n",
    "print(pt2[1:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "- 행렬을 더하거나 뺄 때는 크기가 같아야 함.\n",
    "- 곱셈을 할 때는 A의 마지막 차원과 B의 첫번째 차원의 크기가 같아야 함\n",
    "    - 2차원의 경우, A의 열과 B의 행 크기가 같아야 함\n",
    "- 브로드캐스팅은 자동으로 크기를 맞춰서 연산을 수행\n",
    "- **자동으로 이루어지므로 사용에 주의해야 함.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n",
      "tensor([[3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Vector + scalar\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3]) # [3] -> [3, 3]\n",
    "print(m1 + m2)\n",
    "print(m1 * m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n",
      "tensor([[3., 6.],\n",
      "        [4., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([[3], [4]])\n",
    "print(m1 + m2)\n",
    "print(m1 * m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 경우 브로드캐스팅 과정에서 행렬이 채워짐\n",
    "```\n",
    "[1, 2]\n",
    "==> [[1, 2],\n",
    "     [1, 2]]\n",
    "[3]\n",
    "[4]\n",
    "==> [[3, 3],\n",
    "     [4, 4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬 곱셈의 차이\n",
    "- 행렬 곱셈은 .matmul, 원소 별 곱셈은 .mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank(dim) of m1 is 2\n",
      "Size of m1 is torch.Size([2, 2])\n",
      "Rank(dim) of m2 is 2\n",
      "Size of m2 is torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "Rank(dim) of tensor is 2\n",
      "Size of tensor is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print_info(m1)\n",
    "print_info(m2)\n",
    "print(m1.matmul(m2)) # 2 x 1\n",
    "print_info(m1.matmul(m2)) # 2 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(m1 * m2)\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 총합, 평균, 최대값, 최대값 인덱스\n",
    "- sum, mean, man, argmax\n",
    "- 함수에서 dim 인자를 통해 행 혹은 열을 선택할 수 있음\n",
    "    - 2차원의 경우, dim = 0이면 행을 합쳐버리는 것이고, dim = 1이면 열을 합쳐벼리는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 6.])\n",
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum(dim = 0))\n",
    "print(t.mean(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 7.])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum(dim = 1))\n",
    "print(t.mean(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())\n",
    "print(t.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 5.],\n",
      "        [2., 3.]])\n",
      "tensor([2., 5.])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 5], [2, 3]])\n",
    "\n",
    "print(t)\n",
    "print(t.max(dim = 0)[0])\n",
    "print(t.argmax(dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim = 1)[0])\n",
    "print(t.argmax(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 조작 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 뷰 (중요!?)\n",
    "- 원소의 수를 유지하면서 텐서의 크기를 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n",
      "Rank(dim) of ftv1 is 2\n",
      "Size of ftv1 is torch.Size([4, 1])\n",
      "tensor([[[[1.]]],\n",
      "\n",
      "\n",
      "        [[[2.]]],\n",
      "\n",
      "\n",
      "        [[[3.]]],\n",
      "\n",
      "\n",
      "        [[[4.]]]])\n",
      "Rank(dim) of ftv4 is 4\n",
      "Size of ftv4 is torch.Size([4, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([1, 2, 3, 4])\n",
    "ftv1 = ft.view([-1,1])\n",
    "print(ftv1)\n",
    "print_info(ftv1)\n",
    "ftv4 = ft.view([-1, 1, 1, 1])\n",
    "print(ftv4)\n",
    "print_info(ftv4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "Rank(dim) of ft is 3\n",
      "Size of ft is torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft)\n",
    "print_info(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "Rank(dim) of ftv2 is 2\n",
      "Size of ftv2 is torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "ftv2 = ft.view([-1, 3]) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
    "print(ftv2)\n",
    "print_info(ftv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "Rank(dim) of ftv3 is 3\n",
      "Size of ftv3 is torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "ftv3 = ft.view([-1, 1, 3])\n",
    "print(ftv3)\n",
    "print_info(ftv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.],\n",
      "         [1.],\n",
      "         [2.]]])\n",
      "Rank(dim) of ft is 3\n",
      "Size of ft is torch.Size([1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[[0], [1], [2]]])\n",
    "print(ft)\n",
    "print_info(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "Rank(dim) of fts is 1\n",
      "Size of fts is torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "fts = ft.squeeze()\n",
    "print(fts)\n",
    "print_info(fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "Rank(dim) of ftus is 2\n",
      "Size of ftus is torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ftus = fts.unsqueeze(1)\n",
    "print(ftus)\n",
    "print_info(ftus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "Rank(dim) of ftvus is 2\n",
      "Size of ftvus is torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ftvus = fts.view(-1, 1)\n",
    "print(ftvus)\n",
    "print_info(ftvus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "Rank(dim) of ftv3 is 3\n",
      "Size of ftv3 is torch.Size([4, 1, 3])\n",
      "tensor([[[[[ 0.],\n",
      "           [ 1.],\n",
      "           [ 2.]]],\n",
      "\n",
      "\n",
      "         [[[ 3.],\n",
      "           [ 4.],\n",
      "           [ 5.]]],\n",
      "\n",
      "\n",
      "         [[[ 6.],\n",
      "           [ 7.],\n",
      "           [ 8.]]],\n",
      "\n",
      "\n",
      "         [[[ 9.],\n",
      "           [10.],\n",
      "           [11.]]]]])\n",
      "Rank(dim) of ftv3us is 5\n",
      "Size of ftv3us is torch.Size([1, 4, 1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ftv3)\n",
    "print_info(ftv3)\n",
    "ftv3us = ftv3.unsqueeze(0).unsqueeze(-1)\n",
    "print(ftv3us)\n",
    "print_info(ftv3us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연결, 스택\n",
    "- 연결(cat)은 합쳐버리는 것, 스택은 차원을 늘리는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "print(torch.cat([x, y], dim=0))\n",
    "print(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]],\n",
      "\n",
      "        [[ 9., 10.],\n",
      "         [11., 12.]]])\n",
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]],\n",
      "\n",
      "        [[ 9., 10.],\n",
      "         [11., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "z = torch.FloatTensor([[9, 10], [11, 12]])\n",
    "stacked = torch.stack((x, y, z))\n",
    "print(stacked)\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(stacked))\n",
    "print(torch.zeros_like(stacked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 덮어쓰기 연산\n",
    "- 연산 함수 뒤에 언더바를 붙이면 기존의 값을 덮어쓰기 한다.\n",
    "    - 즉, `x.mul_(2)` 는 `x = x.mul(2)`과 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀 (Linear Regression)\n",
    "- 데이터\n",
    "    - 훈련 데이터와 테스트 데이터\n",
    "- 가설 수립\n",
    "    - 선형 회귀의 가설은 1차 방정식\n",
    "- 손실 계산\n",
    "    - 비용 함수 = 손실 함수 = 오차 함수 = 목적 함수\n",
    "    - cost fn = loss fn = error fn = objective fn\n",
    "    - y의 오차 제곱의 평균. MSE(Mead Squared Error)\n",
    "        - 이건 분산 구하는것과 같구만\n",
    "    - 즉, $$cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$$\n",
    "- 경사 하강법\n",
    "    - 학습률(알파)가 너무 크면 조정 과정에서 발산해 버리고, 너무 작으면 학습이 더뎌진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10992aa90>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "Rank(dim) of x_train is 2\n",
      "Size of x_train is torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "Rank(dim) of y_train is 2\n",
      "Size of y_train is torch.Size([3, 1])\n",
      "tensor([[1., 2.],\n",
      "        [2., 4.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "print(x_train)\n",
    "print_info(x_train)\n",
    "print(y_train)\n",
    "print_info(y_train)\n",
    "\n",
    "print(torch.cat([x_train, y_train], dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)  # 가중치 초기값 0, requres_grad는 학습을 통해 계속 변경되는 값을 의미\n",
    "print(W)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "Epoch    0/2300 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2300 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2300 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2300 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2300 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2300 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2300 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2300 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2300 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2300 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2300 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2300 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2300 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2300 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2300 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2300 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2300 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2300 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2300 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2300 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2300 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch 2100/2300 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch 2200/2300 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch 2300/2300 W: 1.999, b: 0.003 Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True)  # 가중치 초기값 0, requres_grad는 학습을 통해 계속 변경되는 값을 의미\n",
    "print(W)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 0.01)    # lr은 leraning rate\n",
    "\n",
    "nb_epochs = 2300 # 경사 하강법 반복수\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    optimizer.zero_grad()   # gradient를 0으로 초기화\n",
    "    cost.backward() # 비용 함수를 미분하여 gradient 계산\n",
    "    optimizer.step()    # W와 b 업데이트\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자동 미분(Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True) # 값이 2인 임의의 스칼라값 선언\n",
    "# requires_grad = True : 이 텐서의 기울기를 저장하겠다는 의미\n",
    "# 위에서 optimizer.zero_grad를 해주지 않으면 w가 유지됨\n",
    "\n",
    "y = w ** 2\n",
    "z = 2 * y + 5\n",
    "\n",
    "z.backward()\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 선형 회귀\n",
    "- $$H(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.cat([x1_train, x2_train, x3_train], dim = 1)\n",
    "print(x_train)\n",
    "print(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch  100/2000 hypothesis: tensor([152.7691, 183.6985, 180.9591, 197.0627, 140.1336]) Cost: 1.563634\n",
      "Epoch  200/2000 hypothesis: tensor([152.7273, 183.7273, 180.9465, 197.0517, 140.1731]) Cost: 1.497607\n",
      "Epoch  300/2000 hypothesis: tensor([152.6866, 183.7554, 180.9343, 197.0409, 140.2116]) Cost: 1.435026\n",
      "Epoch  400/2000 hypothesis: tensor([152.6470, 183.7827, 180.9224, 197.0304, 140.2491]) Cost: 1.375730\n",
      "Epoch  500/2000 hypothesis: tensor([152.6085, 183.8093, 180.9108, 197.0201, 140.2856]) Cost: 1.319511\n",
      "Epoch  600/2000 hypothesis: tensor([152.5711, 183.8352, 180.8996, 197.0101, 140.3211]) Cost: 1.266222\n",
      "Epoch  700/2000 hypothesis: tensor([152.5346, 183.8604, 180.8887, 197.0003, 140.3558]) Cost: 1.215696\n",
      "Epoch  800/2000 hypothesis: tensor([152.4992, 183.8849, 180.8781, 196.9908, 140.3895]) Cost: 1.167818\n",
      "Epoch  900/2000 hypothesis: tensor([152.4647, 183.9087, 180.8677, 196.9814, 140.4223]) Cost: 1.122429\n",
      "Epoch 1000/2000 hypothesis: tensor([152.4312, 183.9319, 180.8577, 196.9723, 140.4543]) Cost: 1.079378\n",
      "Epoch 1100/2000 hypothesis: tensor([152.3986, 183.9544, 180.8479, 196.9633, 140.4855]) Cost: 1.038584\n",
      "Epoch 1200/2000 hypothesis: tensor([152.3669, 183.9763, 180.8385, 196.9546, 140.5159]) Cost: 0.999894\n",
      "Epoch 1300/2000 hypothesis: tensor([152.3360, 183.9977, 180.8293, 196.9461, 140.5454]) Cost: 0.963217\n",
      "Epoch 1400/2000 hypothesis: tensor([152.3060, 184.0184, 180.8203, 196.9377, 140.5743]) Cost: 0.928421\n",
      "Epoch 1500/2000 hypothesis: tensor([152.2769, 184.0386, 180.8116, 196.9296, 140.6023]) Cost: 0.895453\n",
      "Epoch 1600/2000 hypothesis: tensor([152.2485, 184.0582, 180.8031, 196.9216, 140.6297]) Cost: 0.864161\n",
      "Epoch 1700/2000 hypothesis: tensor([152.2209, 184.0773, 180.7949, 196.9138, 140.6563]) Cost: 0.834503\n",
      "Epoch 1800/2000 hypothesis: tensor([152.1940, 184.0959, 180.7869, 196.9062, 140.6823]) Cost: 0.806375\n",
      "Epoch 1900/2000 hypothesis: tensor([152.1679, 184.1140, 180.7792, 196.8988, 140.7076]) Cost: 0.779696\n",
      "Epoch 2000/2000 hypothesis: tensor([152.1425, 184.1316, 180.7716, 196.8915, 140.7322]) Cost: 0.754389\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((x_train.shape[1], 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.00001)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31826.3926],\n",
      "        [31857.4961],\n",
      "        [31854.1152],\n",
      "        [31870.9355],\n",
      "        [31812.2656]], grad_fn=<SubBackward0>)\n",
      "Epoch    0/200 Cost: 31667.599609\n",
      "tensor([[1.8899],\n",
      "        [1.2077],\n",
      "        [0.6037],\n",
      "        [0.8369],\n",
      "        [0.8668]], grad_fn=<SubBackward0>)\n",
      "Epoch   10/200 Cost: 0.517863\n",
      "tensor([[ 1.1227],\n",
      "        [ 0.3456],\n",
      "        [-0.2508],\n",
      "        [-0.0676],\n",
      "        [ 0.1406]], grad_fn=<SubBackward0>)\n",
      "Epoch   20/200 Cost: 0.227732\n",
      "tensor([[ 1.1202],\n",
      "        [ 0.3442],\n",
      "        [-0.2530],\n",
      "        [-0.0698],\n",
      "        [ 0.1397]], grad_fn=<SubBackward0>)\n",
      "Epoch   30/200 Cost: 0.227505\n",
      "tensor([[ 1.1192],\n",
      "        [ 0.3445],\n",
      "        [-0.2535],\n",
      "        [-0.0701],\n",
      "        [ 0.1401]], grad_fn=<SubBackward0>)\n",
      "Epoch   40/200 Cost: 0.227287\n",
      "tensor([[ 1.1182],\n",
      "        [ 0.3448],\n",
      "        [-0.2540],\n",
      "        [-0.0704],\n",
      "        [ 0.1405]], grad_fn=<SubBackward0>)\n",
      "Epoch   50/200 Cost: 0.227072\n",
      "tensor([[ 1.1172],\n",
      "        [ 0.3451],\n",
      "        [-0.2544],\n",
      "        [-0.0707],\n",
      "        [ 0.1410]], grad_fn=<SubBackward0>)\n",
      "Epoch   60/200 Cost: 0.226848\n",
      "tensor([[ 1.1162],\n",
      "        [ 0.3454],\n",
      "        [-0.2549],\n",
      "        [-0.0711],\n",
      "        [ 0.1414]], grad_fn=<SubBackward0>)\n",
      "Epoch   70/200 Cost: 0.226633\n",
      "tensor([[ 1.1152],\n",
      "        [ 0.3457],\n",
      "        [-0.2554],\n",
      "        [-0.0714],\n",
      "        [ 0.1418]], grad_fn=<SubBackward0>)\n",
      "Epoch   80/200 Cost: 0.226421\n",
      "tensor([[ 1.1142],\n",
      "        [ 0.3461],\n",
      "        [-0.2558],\n",
      "        [-0.0717],\n",
      "        [ 0.1422]], grad_fn=<SubBackward0>)\n",
      "Epoch   90/200 Cost: 0.226207\n",
      "tensor([[ 1.1132],\n",
      "        [ 0.3464],\n",
      "        [-0.2563],\n",
      "        [-0.0720],\n",
      "        [ 0.1426]], grad_fn=<SubBackward0>)\n",
      "Epoch  100/200 Cost: 0.225993\n",
      "tensor([[ 1.1122],\n",
      "        [ 0.3467],\n",
      "        [-0.2568],\n",
      "        [-0.0723],\n",
      "        [ 0.1430]], grad_fn=<SubBackward0>)\n",
      "Epoch  110/200 Cost: 0.225780\n",
      "tensor([[ 1.1112],\n",
      "        [ 0.3470],\n",
      "        [-0.2572],\n",
      "        [-0.0726],\n",
      "        [ 0.1434]], grad_fn=<SubBackward0>)\n",
      "Epoch  120/200 Cost: 0.225562\n",
      "tensor([[ 1.1102],\n",
      "        [ 0.3473],\n",
      "        [-0.2577],\n",
      "        [-0.0729],\n",
      "        [ 0.1438]], grad_fn=<SubBackward0>)\n",
      "Epoch  130/200 Cost: 0.225359\n",
      "tensor([[ 1.1092],\n",
      "        [ 0.3476],\n",
      "        [-0.2581],\n",
      "        [-0.0732],\n",
      "        [ 0.1443]], grad_fn=<SubBackward0>)\n",
      "Epoch  140/200 Cost: 0.225144\n",
      "tensor([[ 1.1082],\n",
      "        [ 0.3479],\n",
      "        [-0.2586],\n",
      "        [-0.0735],\n",
      "        [ 0.1447]], grad_fn=<SubBackward0>)\n",
      "Epoch  150/200 Cost: 0.224938\n",
      "tensor([[ 1.1072],\n",
      "        [ 0.3483],\n",
      "        [-0.2590],\n",
      "        [-0.0739],\n",
      "        [ 0.1451]], grad_fn=<SubBackward0>)\n",
      "Epoch  160/200 Cost: 0.224725\n",
      "tensor([[ 1.1063],\n",
      "        [ 0.3486],\n",
      "        [-0.2595],\n",
      "        [-0.0741],\n",
      "        [ 0.1455]], grad_fn=<SubBackward0>)\n",
      "Epoch  170/200 Cost: 0.224525\n",
      "tensor([[ 1.1053],\n",
      "        [ 0.3489],\n",
      "        [-0.2599],\n",
      "        [-0.0744],\n",
      "        [ 0.1459]], grad_fn=<SubBackward0>)\n",
      "Epoch  180/200 Cost: 0.224316\n",
      "tensor([[ 1.1044],\n",
      "        [ 0.3492],\n",
      "        [-0.2604],\n",
      "        [-0.0747],\n",
      "        [ 0.1463]], grad_fn=<SubBackward0>)\n",
      "Epoch  190/200 Cost: 0.224116\n",
      "tensor([[ 1.1034],\n",
      "        [ 0.3495],\n",
      "        [-0.2609],\n",
      "        [-0.0750],\n",
      "        [ 0.1467]], grad_fn=<SubBackward0>)\n",
      "Epoch  200/200 Cost: 0.223911\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = nn.Linear(3,1)\n",
    "\n",
    "# print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 200\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "        print(y_train - (prediction - cost))\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2306]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27acd57041ab43bd7420ea3701530ac139f1b2fc5b72b7f2b8dcb4a7eb4acbfb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv-mac': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
